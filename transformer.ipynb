{"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"","display_name":""},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_dest, n_src, dtype):\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype = tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\nnp.transpose(causal_attention_mask(1, 10, 10, dtype = tf.int32)[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(layers.Layers):\n    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate = 0.1):\n        super(TransformerBlock, self).__init__()\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.embed_dim = embed_dim\n        self.ff_dim = ff_dim\n        self.dropout_rate = dropout_rate\n        self.attn = layers.MultiHeadAttention(\n            num_heads, key_dim, output_shape = embed_dim\n        )\n        self.dropout_1 = layers.Dropout(self.dropout_rate)\n        self.ln_1 = layers.LayerNormalization(epsilon = 1e-6)\n        self.ffn_1 = layers.Dense(self.ff_dim, activation = \"relu\")\n        self.ffn_2 = layers.Dense(self.embed_dim)\n        self.dropout_2 = layers.Dropoout(self.dropout_rate)\n        self.ln_2 = layers.LayerNormalization(epsilon = 1e-6)\n    \n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(\n            batch_size, seq_len, seq_len, tf.bool\n        )\n        attention_output, attention_scores = self.attn(\n            inputs, inputs, attention_mask = causal_mask, return_attention_scores = True\n        )\n        attention_output = self.dropout_1(attention_output)\n        out1 = self.ln_1(inputs + attention_output)\n        ffn_1 = self.ffn_1(out1)\n        ffn_2 = self.ffn_2(ffn_1)\n        ffn_output = self.dropout_2(ffn_2)\n        return(self.ln_2(out1 + ffn_output), attention_scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.maxlen = maxlen\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.token_emb = layers.Embedding(\n            input_dim = vocab_size, output_dim = embed_dim\n        )\n        self.pos_mb = layers.Embedding(input_dim = maxlen, output_dim = embed_dim)\n    \n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start = 0, limit = maxlen, delta = 1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 80\nVOCAB_SIZE = 10000\nEMBEDDING_DIM = 256\nN_HEADS = 2\nKEY_DIM = 256\nFEED_FORWARD_DIM = 256\n\ninputs = layers.Input(shape = (None,), dtype = tf.int32)\nx = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\nx, attention_scores = TransformerBlock(\n    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n)(x)\noutputs = layers.Dense(VOCAB_SIZE, activation = 'softmax')(x)\ngpt = models.Model(inputs = inputs, outputs = [outputs, attention])\ngpt.compile(\"adam\", loss = [losses.SparseCategoricalCrossentropy(), None])\ngpt.fit(train_ds, epochs = 5)","metadata":{},"execution_count":null,"outputs":[]}]}